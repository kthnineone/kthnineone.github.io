---
layout: post
title:  "NLP Seq2Seq Models"
date:   2023-11-29 00:11:00 +0900
categories: NLP DeepLearning
tags: nlp seq2seq pre-trained-model
use_math: true
---
<h4> NLP Seq2Seq Models </h4>  
  
아래의 세 가지 자료를 참고하여 주요 NLP Models를 정리해보려고 합니다.  
<br> 
<br>  
  

<h5> 1. BERT and Related Models Map </h5> 

![Image Alt 텍스트]({{site.url}}/assets/images/seq2seq_BERT_related_Models.png )
<br>

BERT를 중심으로 연관된 여러 Sequence to Sequence Model의 관계도입니다.  
여기 나온 모델들을 정리해보면 다음과 같습니다.  
<br>

+ ELMo
+ BERT
+ GPT
+ GPT-2
+ Grover
+ ULMFiT
+ XLM UDify
+ MT-DNN
+ MT-DNN_KD
+ MASS UniLM
+ SpanBERT
+ RoBERTa
+ XLNet
+ ERINE (Tsinghua)
+ KnowBert
+ VideoBERT
+ CBT
+ ViLBERT
+ VisualBERT
+ B2T2
+ Unicoder-VL
+ LXMERT
+ VL-BERT
+ UNITER
+ ERINE (Baidu)
+ BERT-wwm

<br>


<h5> 2. Korean Pre-Trained seq2seq Models </h5> 

https://sooftware.io/korean-plm/
<br>

위 사이트에서 한국어 사전학습 모델들이 나와있는데 여기서 쓰이는 주요 기본 모델들을 정리해보았습니다.  

<br>

+ BERT
+ RoBERTa
+ ELECTRA
+ ALBERT
+ Funnel (Transformer)
+ BigBird
+ GPT-2
+ GPT-3
+ BART
+ T5

<br>
<br>
  

<h5> 3. Recent Survey Paper </h5>  
Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey, Bonan Min et al, 2023.  
위 서베이 논문을 참고하여 적었습니다.  

+ BERT
+ RoBERTa
+ XLNet
+ GPT
+ GPT-2
+ GPT-3
+ BART
+ T5
+ <small>M</small>BERT
+ XLM-R
+ <small>M</small>T5

<br>
<br>
  

<h5> 4. 주요 모델들 </h5>  
  
많이 겹치는 모델들을 정리하고  
OpenAi의 GTP-3.5와 GPT-4와  
Google의 PaLM 1, 2, Bard, Minerva,  
Meta의 LLaMa-1과 2를 추가해서 정리했습니다.  

+ RNN
+ LSTM
+ GRU (2014)
+ Transformer (2017)
+ ELMo (2018)
+ BERT (2018)
+ GPT (2018)
+ RoBERTa (2019)
+ ALBERT (2019)
+ BART (2019)
+ T5 (2019)
+ GPT-2 (2019)
+ XLNet (2019)
+ ELECTRA (2020)
+ GPT-3 (2020)
+ GPT-3.5 (2022)
+ PaLM (2022)
+ Minerva (2022)
+ GPT-4 (2023)
+ Bard (2023)
+ PaLM 2 (2023)
+ LLaMA-1 (2023)
+ LLaMA-2 (2023)

<br>
이미지 출처: https://blog.hsutimes.com/2019/10/20/PLMpapers/
<br>

