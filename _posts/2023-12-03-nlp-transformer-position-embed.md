---
layout: post
title:  "Transformer Positional Embedding"
date:   2023-12-03 00:08:00 +0900
categories: NLP DeepLearning
tags: nlp seq2seq embedding
use_math: true
---
<h3> Transformer Positional Embedding </h3>  
  
Positional Embedding이란 tokens의 배열 순서를 반영하기 위한 embedding  

<br> 
<br>  


Ref 1:  https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer5-Positional-Encoding  
Ref 2: https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80  
Ref 3: https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding  
